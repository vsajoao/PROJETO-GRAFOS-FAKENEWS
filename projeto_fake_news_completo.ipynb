{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b6588b",
   "metadata": {},
   "source": [
    "\n",
    "# An√°lise de Redes Sociais: Detec√ß√£o de Espalhadores de Fake News\n",
    "\n",
    "**Disciplina:** Teoria e Aplica√ß√£o de Grafos - 2025/1  \n",
    "**Professor:** D√≠bio  \n",
    "**Alunos:** [Nomes e Matr√≠culas]\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "Analisar uma rede social (Twitter - Dataset Higgs) e identificar potenciais espalhadores de fake news usando algoritmos de grafos.\n",
    "\n",
    "Ser√£o aplicadas medidas de centralidade, detec√ß√£o de comunidades e o algoritmo de PageRank para entender a estrutura da rede e detectar n√≥s mais influentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf984f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from networkx.algorithms.community import greedy_modularity_communities, girvan_newman\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ‚ö° CONFIGURA√á√ÉO DE VELOCIDADE\n",
    "FAST_MODE = True  # Mude para False se quiser an√°lise completa (mais lenta)\n",
    "MAX_NODES = 1000 if FAST_MODE else 10000\n",
    "MAX_EDGES = 3000 if FAST_MODE else 50000\n",
    "\n",
    "print(f\"üöÄ Modo: {'R√ÅPIDO' if FAST_MODE else 'COMPLETO'}\")\n",
    "print(f\"Limites: {MAX_NODES} n√≥s, {MAX_EDGES} arestas\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def load_edgelist(filename):\n",
    "    \"\"\"Load and process edgelist from gzipped file\"\"\"\n",
    "    try:\n",
    "        with gzip.open(filename, 'rt') as f:\n",
    "            data = [line.strip().split() for line in f]\n",
    "        df = pd.DataFrame(data, columns=['user1', 'user2', 'weight'])\n",
    "        df['weight'] = df['weight'].astype(int)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Using sample data for demonstration.\")\n",
    "        return generate_sample_data()\n",
    "\n",
    "def generate_sample_data():\n",
    "    \"\"\"Generate sample Twitter-like network data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_users = 300 if FAST_MODE else 500\n",
    "    n_edges = 1000 if FAST_MODE else 2000\n",
    "    \n",
    "    users = [f\"user_{i}\" for i in range(n_users)]\n",
    "    \n",
    "    # Create realistic network structure\n",
    "    edges = []\n",
    "    for i in range(n_edges):\n",
    "        user1 = np.random.choice(users)\n",
    "        user2 = np.random.choice(users)\n",
    "        if user1 != user2:\n",
    "            weight = np.random.randint(1, 10)\n",
    "            edges.append([user1, user2, weight])\n",
    "    \n",
    "    return pd.DataFrame(edges, columns=['user1', 'user2', 'weight'])\n",
    "\n",
    "# =========================================================\n",
    "# ETAPA 1: COLETA E PREPARA√á√ÉO DOS DADOS\n",
    "# =========================================================\n",
    "print(\"=== ETAPA 1: COLETA E PREPARA√á√ÉO DOS DADOS ===\")\n",
    "\n",
    "# Tentar carregar dados reais, usar dados de exemplo se n√£o encontrar\n",
    "try:\n",
    "    df_retweet = load_edgelist('higgs-retweet_network.edgelist.gz')\n",
    "    df_reply = load_edgelist('higgs-reply_network.edgelist.gz')\n",
    "    df_mention = load_edgelist('higgs-mention_network.edgelist.gz')\n",
    "    print(\"Dados reais carregados com sucesso!\")\n",
    "except:\n",
    "    print(\"Usando dados de exemplo para demonstra√ß√£o...\")\n",
    "    df_retweet = generate_sample_data()\n",
    "    df_reply = generate_sample_data()\n",
    "    df_mention = generate_sample_data()\n",
    "\n",
    "# Juntar todos os dados\n",
    "df_all = pd.concat([df_retweet, df_reply, df_mention])\n",
    "df_all = df_all.groupby(['user1', 'user2']).sum().reset_index()\n",
    "\n",
    "# OTIMIZA√á√ÉO: Filtrar apenas intera√ß√µes com peso significativo e limitar tamanho\n",
    "min_weight = 2  # Filtrar intera√ß√µes fracas\n",
    "df_all = df_all[df_all['weight'] >= min_weight]\n",
    "\n",
    "# Limitar a um subconjunto se muito grande\n",
    "if len(df_all) > MAX_EDGES:\n",
    "    print(f\"Dataset muito grande ({len(df_all)} arestas). Usando subset de {MAX_EDGES} arestas com maiores pesos.\")\n",
    "    df_all = df_all.nlargest(MAX_EDGES, 'weight')\n",
    "\n",
    "# Filtrar usu√°rios com poucas conex√µes para reduzir ru√≠do\n",
    "user_counts = pd.concat([df_all['user1'], df_all['user2']]).value_counts()\n",
    "active_users = user_counts[user_counts >= 2].index.tolist()  # Pelo menos 2 intera√ß√µes\n",
    "df_all = df_all[df_all['user1'].isin(active_users) & df_all['user2'].isin(active_users)]\n",
    "\n",
    "print(f\"Total de intera√ß√µes ap√≥s filtros: {len(df_all)}\")\n",
    "print(f\"Usu√°rios √∫nicos: {len(set(df_all['user1'].tolist() + df_all['user2'].tolist()))}\")\n",
    "\n",
    "# =========================================================\n",
    "# ETAPA 2: CONSTRU√á√ÉO DO GRAFO\n",
    "# =========================================================\n",
    "print(\"\\n=== ETAPA 2: CONSTRU√á√ÉO DO GRAFO ===\")\n",
    "\n",
    "# Criar grafo direcionado\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Adicionar arestas com pesos\n",
    "for _, row in df_all.iterrows():\n",
    "    G.add_edge(row['user1'], row['user2'], weight=row['weight'])\n",
    "\n",
    "print(f\"N√∫mero de n√≥s: {G.number_of_nodes()}\")\n",
    "print(f\"N√∫mero de arestas: {G.number_of_edges()}\")\n",
    "print(f\"Densidade do grafo: {nx.density(G):.6f}\")\n",
    "\n",
    "# =========================================================\n",
    "# ETAPA 3: ALGORITMOS DE DETEC√á√ÉO\n",
    "# =========================================================\n",
    "print(\"\\n=== ETAPA 3: AN√ÅLISE COM ALGORITMOS DE GRAFOS ===\")\n",
    "\n",
    "# 3.1 PAGERANK - Identificar usu√°rios mais influentes\n",
    "print(\"\\n3.1 Calculando PageRank...\")\n",
    "pagerank_scores = nx.pagerank(G, alpha=0.85, weight='weight')\n",
    "top_pagerank = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Top 10 usu√°rios mais influentes (PageRank):\")\n",
    "for i, (user, score) in enumerate(top_pagerank, 1):\n",
    "    print(f\"{i}. {user}: {score:.6f}\")\n",
    "\n",
    "# 3.2 DETEC√á√ÉO DE COMUNIDADES\n",
    "print(\"\\n3.2 Detectando comunidades...\")\n",
    "# Usar grafo n√£o-direcionado para detec√ß√£o de comunidades\n",
    "G_undirected = G.to_undirected()\n",
    "\n",
    "# OTIMIZA√á√ÉO: Usar apenas componente gigante se grafo for desconectado\n",
    "if not nx.is_connected(G_undirected):\n",
    "    # Pegar apenas o maior componente conectado\n",
    "    largest_cc = max(nx.connected_components(G_undirected), key=len)\n",
    "    G_undirected = G_undirected.subgraph(largest_cc).copy()\n",
    "    print(f\"Usando maior componente conectado: {len(largest_cc)} n√≥s\")\n",
    "\n",
    "communities = list(greedy_modularity_communities(G_undirected))\n",
    "\n",
    "print(f\"N√∫mero de comunidades detectadas: {len(communities)}\")\n",
    "print(\"Tamanho das 5 maiores comunidades:\")\n",
    "community_sizes = sorted([len(c) for c in communities], reverse=True)\n",
    "for i, size in enumerate(community_sizes[:5], 1):\n",
    "    print(f\"Comunidade {i}: {size} usu√°rios\")\n",
    "\n",
    "# 3.3 MEDIDAS DE CENTRALIDADE\n",
    "print(\"\\n3.3 Calculando medidas de centralidade...\")\n",
    "\n",
    "# Centralidade de grau (r√°pida)\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# OTIMIZA√á√ÉO: Calcular betweenness apenas para subset de n√≥s importantes se grafo for muito grande\n",
    "if G.number_of_nodes() > 1000:\n",
    "    print(\"Grafo grande detectado. Calculando betweenness para top 200 n√≥s por grau...\")\n",
    "    # Pegar apenas os top n√≥s por grau para calcular betweenness\n",
    "    top_nodes_by_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:200]\n",
    "    important_nodes = [node for node, _ in top_nodes_by_degree]\n",
    "    betweenness_centrality = nx.betweenness_centrality_subset(G, sources=important_nodes, targets=important_nodes, weight='weight')\n",
    "else:\n",
    "    print(\"Calculando centralidade de intermedia√ß√£o...\")\n",
    "    betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "\n",
    "top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# OTIMIZA√á√ÉO: Closeness centralidade tamb√©m pode ser limitada\n",
    "if G.number_of_nodes() > 1000:\n",
    "    print(\"Calculando closeness centrality para subset...\")\n",
    "    closeness_centrality = {}\n",
    "    for node in important_nodes:\n",
    "        try:\n",
    "            closeness_centrality[node] = nx.closeness_centrality(G, u=node, distance='weight')\n",
    "        except:\n",
    "            closeness_centrality[node] = 0\n",
    "else:\n",
    "    print(\"Calculando centralidade de proximidade...\")\n",
    "    closeness_centrality = nx.closeness_centrality(G, distance='weight')\n",
    "\n",
    "top_closeness = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(\"\\nTop 5 por Centralidade de Grau:\")\n",
    "for user, score in top_degree:\n",
    "    print(f\"  {user}: {score:.6f}\")\n",
    "\n",
    "print(\"\\nTop 5 por Centralidade de Intermedia√ß√£o:\")\n",
    "for user, score in top_betweenness:\n",
    "    print(f\"  {user}: {score:.6f}\")\n",
    "\n",
    "print(\"\\nTop 5 por Centralidade de Proximidade:\")\n",
    "for user, score in top_closeness:\n",
    "    print(f\"  {user}: {score:.6f}\")\n",
    "\n",
    "# =========================================================\n",
    "# ETAPA 4: IDENTIFICA√á√ÉO DE POTENCIAIS ESPALHADORES\n",
    "# =========================================================\n",
    "print(\"\\n=== ETAPA 4: IDENTIFICA√á√ÉO DE ESPALHADORES DE FAKE NEWS ===\")\n",
    "\n",
    "# Combinar m√©tricas para identificar potenciais espalhadores\n",
    "def identify_potential_spreaders(pagerank_dict, degree_dict, betweenness_dict, top_n=10):\n",
    "    \"\"\"Identifica potenciais espalhadores baseado em m√∫ltiplas m√©tricas\"\"\"\n",
    "    all_users = set(pagerank_dict.keys())\n",
    "    \n",
    "    # Normalizar scores\n",
    "    max_pr = max(pagerank_dict.values())\n",
    "    max_deg = max(degree_dict.values())\n",
    "    max_bet = max(betweenness_dict.values())\n",
    "    \n",
    "    combined_scores = {}\n",
    "    for user in all_users:\n",
    "        pr_norm = pagerank_dict[user] / max_pr\n",
    "        deg_norm = degree_dict[user] / max_deg\n",
    "        bet_norm = betweenness_dict[user] / max_bet if max_bet > 0 else 0\n",
    "        \n",
    "        # Score combinado (pode ajustar os pesos)\n",
    "        combined_scores[user] = 0.4 * pr_norm + 0.3 * deg_norm + 0.3 * bet_norm\n",
    "    \n",
    "    return sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "potential_spreaders = identify_potential_spreaders(\n",
    "    pagerank_scores, degree_centrality, betweenness_centrality\n",
    ")\n",
    "\n",
    "print(\"Top 10 Potenciais Espalhadores de Fake News:\")\n",
    "for i, (user, score) in enumerate(potential_spreaders, 1):\n",
    "    print(f\"{i}. {user}: Score combinado {score:.6f}\")\n",
    "\n",
    "# =========================================================\n",
    "# ETAPA 5: VISUALIZA√á√ïES\n",
    "# =========================================================\n",
    "print(\"\\n=== ETAPA 5: VISUALIZA√á√ïES ===\")\n",
    "\n",
    "# 5.1 Distribui√ß√£o de graus\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Distribui√ß√£o de graus\n",
    "plt.subplot(2, 3, 1)\n",
    "degrees = [d for n, d in G.degree()]\n",
    "plt.hist(degrees, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribui√ß√£o de Graus')\n",
    "plt.xlabel('Grau')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Subplot 2: Distribui√ß√£o de PageRank\n",
    "plt.subplot(2, 3, 2)\n",
    "pr_values = list(pagerank_scores.values())\n",
    "plt.hist(pr_values, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "plt.title('Distribui√ß√£o PageRank')\n",
    "plt.xlabel('Score PageRank')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Subplot 3: Top usu√°rios por PageRank\n",
    "plt.subplot(2, 3, 3)\n",
    "users_pr = [user[:10] + '...' for user, _ in top_pagerank[:5]]\n",
    "scores_pr = [score for _, score in top_pagerank[:5]]\n",
    "plt.barh(users_pr, scores_pr)\n",
    "plt.title('Top 5 PageRank')\n",
    "plt.xlabel('Score')\n",
    "\n",
    "# Subplot 4: Centralidades comparadas\n",
    "plt.subplot(2, 3, 4)\n",
    "top_users = [user for user, _ in top_pagerank[:5]]\n",
    "deg_scores = [degree_centrality[user] for user in top_users]\n",
    "bet_scores = [betweenness_centrality[user] for user in top_users]\n",
    "close_scores = [closeness_centrality[user] for user in top_users]\n",
    "\n",
    "x = np.arange(len(top_users))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, deg_scores, width, label='Grau', alpha=0.8)\n",
    "plt.bar(x, bet_scores, width, label='Intermedia√ß√£o', alpha=0.8)\n",
    "plt.bar(x + width, close_scores, width, label='Proximidade', alpha=0.8)\n",
    "\n",
    "plt.title('Compara√ß√£o de Centralidades')\n",
    "plt.xlabel('Usu√°rios (Top PageRank)')\n",
    "plt.ylabel('Score de Centralidade')\n",
    "plt.xticks(x, [u[:8] + '...' for u in top_users], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 5: Tamanho das comunidades\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.bar(range(1, min(11, len(community_sizes)+1)), community_sizes[:10])\n",
    "plt.title('Tamanho das Comunidades')\n",
    "plt.xlabel('Comunidade')\n",
    "plt.ylabel('N√∫mero de Usu√°rios')\n",
    "\n",
    "# Subplot 6: Rede das principais intera√ß√µes\n",
    "plt.subplot(2, 3, 6)\n",
    "# Criar subgrafo com apenas os top usu√°rios\n",
    "top_20_users = [user for user, _ in potential_spreaders[:20]]\n",
    "G_sub = G.subgraph(top_20_users)\n",
    "\n",
    "pos = nx.spring_layout(G_sub, k=1, iterations=50)\n",
    "node_sizes = [pagerank_scores[node] * 10000 for node in G_sub.nodes()]\n",
    "\n",
    "nx.draw(G_sub, pos, node_size=node_sizes, node_color='red', \n",
    "        alpha=0.6, with_labels=False, edge_color='gray', arrows=True)\n",
    "plt.title('Rede dos Principais Espalhadores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =========================================================\n",
    "# ETAPA 6: AN√ÅLISE DE RESULTADOS\n",
    "# =========================================================\n",
    "print(\"\\n=== ETAPA 6: AN√ÅLISE DE RESULTADOS ===\")\n",
    "\n",
    "print(f\"\\nüìä RESUMO DA AN√ÅLISE:\")\n",
    "print(f\"‚Ä¢ Rede analisada com {G.number_of_nodes()} usu√°rios e {G.number_of_edges()} intera√ß√µes\")\n",
    "print(f\"‚Ä¢ Densidade da rede: {nx.density(G):.6f} (rede esparsa)\")\n",
    "print(f\"‚Ä¢ {len(communities)} comunidades detectadas\")\n",
    "print(f\"‚Ä¢ Usu√°rio mais influente (PageRank): {top_pagerank[0][0]}\")\n",
    "print(f\"‚Ä¢ Maior intermediador: {top_betweenness[0][0]}\")\n",
    "\n",
    "print(f\"\\nüéØ POTENCIAIS ESPALHADORES IDENTIFICADOS:\")\n",
    "print(\"Os usu√°rios com maior potencial para espalhar fake news s√£o aqueles que combinam:\")\n",
    "print(\"‚Ä¢ Alto PageRank (influ√™ncia)\")\n",
    "print(\"‚Ä¢ Alta centralidade de grau (muitas conex√µes)\")\n",
    "print(\"‚Ä¢ Alta centralidade de intermedia√ß√£o (ponte entre comunidades)\")\n",
    "\n",
    "print(f\"\\nüí° INSIGHTS PARA DETEC√á√ÉO DE FAKE NEWS:\")\n",
    "print(\"1. Monitorar usu√°rios com score combinado alto\")\n",
    "print(\"2. Focar em usu√°rios que conectam diferentes comunidades\")\n",
    "print(\"3. Analisar padr√µes de dissemina√ß√£o r√°pida\")\n",
    "print(\"4. Verificar usu√°rios com muitos retweets/mentions\")\n",
    "\n",
    "# Salvar resultados principais\n",
    "results_df = pd.DataFrame({\n",
    "    'user': [user for user, _ in potential_spreaders],\n",
    "    'combined_score': [score for _, score in potential_spreaders],\n",
    "    'pagerank': [pagerank_scores[user] for user, _ in potential_spreaders],\n",
    "    'degree_centrality': [degree_centrality[user] for user, _ in potential_spreaders],\n",
    "    'betweenness_centrality': [betweenness_centrality[user] for user, _ in potential_spreaders]\n",
    "})\n",
    "\n",
    "results_df.to_csv('potential_fake_news_spreaders.csv', index=False)\n",
    "print(f\"\\n‚úÖ Resultados salvos em 'potential_fake_news_spreaders.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AN√ÅLISE COMPLETA! üéâ\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcb6c16",
   "metadata": {},
   "source": [
    "\n",
    "## Conclus√µes\n",
    "\n",
    "- A rede possui baixa densidade, o que √© t√≠pico em redes sociais.\n",
    "- Foram detectadas v√°rias comunidades, indicando agrupamentos naturais.\n",
    "- O PageRank, as centralidades de grau e de intermedia√ß√£o foram fundamentais para identificar os usu√°rios mais influentes.\n",
    "- A exporta√ß√£o dos resultados permite an√°lise no Gephi e facilita estudos posteriores.\n",
    "\n",
    "---\n",
    "\n",
    "**An√°lise completa finalizada.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
