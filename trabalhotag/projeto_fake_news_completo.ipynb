{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8b6588b",
   "metadata": {},
   "source": [
    "\n",
    "# Análise de Redes Sociais: Detecção de Espalhadores de Fake News\n",
    "\n",
    "**Disciplina:** Teoria e Aplicação de Grafos - 2025/1  \n",
    "**Professor:** Díbio  \n",
    "**Alunos:** [Nomes e Matrículas]\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo\n",
    "Analisar uma rede social (Twitter - Dataset Higgs) e identificar potenciais espalhadores de fake news usando algoritmos de grafos.\n",
    "\n",
    "Serão aplicadas medidas de centralidade, detecção de comunidades e o algoritmo de PageRank para entender a estrutura da rede e detectar nós mais influentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf984f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from networkx.algorithms.community import greedy_modularity_communities, girvan_newman\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ⚡ CONFIGURAÇÃO DE VELOCIDADE\n",
    "FAST_MODE = True  # Mude para False se quiser análise completa (mais lenta)\n",
    "MAX_NODES = 1000 if FAST_MODE else 10000\n",
    "MAX_EDGES = 3000 if FAST_MODE else 50000\n",
    "\n",
    "print(f\"🚀 Modo: {'RÁPIDO' if FAST_MODE else 'COMPLETO'}\")\n",
    "print(f\"Limites: {MAX_NODES} nós, {MAX_EDGES} arestas\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def load_edgelist(filename):\n",
    "    \"\"\"Load and process edgelist from gzipped file\"\"\"\n",
    "    try:\n",
    "        with gzip.open(filename, 'rt') as f:\n",
    "            data = [line.strip().split() for line in f]\n",
    "        df = pd.DataFrame(data, columns=['user1', 'user2', 'weight'])\n",
    "        df['weight'] = df['weight'].astype(int)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {filename} not found. Using sample data for demonstration.\")\n",
    "        return generate_sample_data()\n",
    "\n",
    "def generate_sample_data():\n",
    "    \"\"\"Generate sample Twitter-like network data for demonstration\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_users = 300 if FAST_MODE else 500\n",
    "    n_edges = 1000 if FAST_MODE else 2000\n",
    "    \n",
    "    users = [f\"user_{i}\" for i in range(n_users)]\n",
    "    \n",
    "    # Create realistic network structure\n",
    "    edges = []\n",
    "    for i in range(n_edges):\n",
    "        user1 = np.random.choice(users)\n",
    "        user2 = np.random.choice(users)\n",
    "        if user1 != user2:\n",
    "            weight = np.random.randint(1, 10)\n",
    "            edges.append([user1, user2, weight])\n",
    "    \n",
    "    return pd.DataFrame(edges, columns=['user1', 'user2', 'weight'])\n",
    "\n",
    "# =========================================================\n",
    "# ETAPA 1: COLETA E PREPARAÇÃO DOS DADOS\n",
    "# =========================================================\n",
    "print(\"=== ETAPA 1: COLETA E PREPARAÇÃO DOS DADOS ===\")\n",
    "\n",
    "# Tentar carregar dados reais, usar dados de exemplo se não encontrar\n",
    "try:\n",
    "    df_retweet = load_edgelist('higgs-retweet_network.edgelist.gz')\n",
    "    df_reply = load_edgelist('higgs-reply_network.edgelist.gz')\n",
    "    df_mention = load_edgelist('higgs-mention_network.edgelist.gz')\n",
    "    print(\"Dados reais carregados com sucesso!\")\n",
    "except:\n",
    "    print(\"Usando dados de exemplo para demonstração...\")\n",
    "    df_retweet = generate_sample_data()\n",
    "    df_reply = generate_sample_data()\n",
    "    df_mention = generate_sample_data()\n",
    "\n",
    "# Juntar todos os dados\n",
    "df_all = pd.concat([df_retweet, df_reply, df_mention])\n",
    "df_all = df_all.groupby(['user1', 'user2']).sum().reset_index()\n",
    "\n",
    "# OTIMIZAÇÃO: Filtrar apenas interações com peso significativo e limitar tamanho\n",
    "min_weight = 2  # Filtrar interações fracas\n",
    "df_all = df_all[df_all['weight'] >= min_weight]\n",
    "\n",
    "# Limitar a um subconjunto se muito grande\n",
    "if len(df_all) > MAX_EDGES:\n",
    "    print(f\"Dataset muito grande ({len(df_all)} arestas). Usando subset de {MAX_EDGES} arestas com maiores pesos.\")\n",
    "    df_all = df_all.nlargest(MAX_EDGES, 'weight')\n",
    "\n",
    "# Filtrar usuários com poucas conexões para reduzir ruído\n",
    "user_counts = pd.concat([df_all['user1'], df_all['user2']]).value_counts()\n",
    "active_users = user_counts[user_counts >= 2].index.tolist()  # Pelo menos 2 interações\n",
    "df_all = df_all[df_all['user1'].isin(active_users) & df_all['user2'].isin(active_users)]\n",
    "\n",
    "print(f\"Total de interações após filtros: {len(df_all)}\")\n",
    "print(f\"Usuários únicos: {len(set(df_all['user1'].tolist() + df_all['user2'].tolist()))}\")\n",
    "\n",
    "# =========================================================\n",
    "# ETAPA 2: CONSTRUÇÃO DO GRAFO\n",
    "# =========================================================\n",
    "print(\"\\n=== ETAPA 2: CONSTRUÇÃO DO GRAFO ===\")\n",
    "\n",
    "# Criar grafo direcionado\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Adicionar arestas com pesos\n",
    "for _, row in df_all.iterrows():\n",
    "    G.add_edge(row['user1'], row['user2'], weight=row['weight'])\n",
    "\n",
    "print(f\"Número de nós: {G.number_of_nodes()}\")\n",
    "print(f\"Número de arestas: {G.number_of_edges()}\")\n",
    "print(f\"Densidade do grafo: {nx.density(G):.6f}\")\n",
    "\n",
    "# =========================================================\n",
    "# ETAPA 3: ALGORITMOS DE DETECÇÃO\n",
    "# =========================================================\n",
    "print(\"\\n=== ETAPA 3: ANÁLISE COM ALGORITMOS DE GRAFOS ===\")\n",
    "\n",
    "# 3.1 PAGERANK - Identificar usuários mais influentes\n",
    "print(\"\\n3.1 Calculando PageRank...\")\n",
    "pagerank_scores = nx.pagerank(G, alpha=0.85, weight='weight')\n",
    "top_pagerank = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"Top 10 usuários mais influentes (PageRank):\")\n",
    "for i, (user, score) in enumerate(top_pagerank, 1):\n",
    "    print(f\"{i}. {user}: {score:.6f}\")\n",
    "\n",
    "# 3.2 DETECÇÃO DE COMUNIDADES\n",
    "print(\"\\n3.2 Detectando comunidades...\")\n",
    "# Usar grafo não-direcionado para detecção de comunidades\n",
    "G_undirected = G.to_undirected()\n",
    "\n",
    "# OTIMIZAÇÃO: Usar apenas componente gigante se grafo for desconectado\n",
    "if not nx.is_connected(G_undirected):\n",
    "    # Pegar apenas o maior componente conectado\n",
    "    largest_cc = max(nx.connected_components(G_undirected), key=len)\n",
    "    G_undirected = G_undirected.subgraph(largest_cc).copy()\n",
    "    print(f\"Usando maior componente conectado: {len(largest_cc)} nós\")\n",
    "\n",
    "communities = list(greedy_modularity_communities(G_undirected))\n",
    "\n",
    "print(f\"Número de comunidades detectadas: {len(communities)}\")\n",
    "print(\"Tamanho das 5 maiores comunidades:\")\n",
    "community_sizes = sorted([len(c) for c in communities], reverse=True)\n",
    "for i, size in enumerate(community_sizes[:5], 1):\n",
    "    print(f\"Comunidade {i}: {size} usuários\")\n",
    "\n",
    "# 3.3 MEDIDAS DE CENTRALIDADE\n",
    "print(\"\\n3.3 Calculando medidas de centralidade...\")\n",
    "\n",
    "# Centralidade de grau (rápida)\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# OTIMIZAÇÃO: Calcular betweenness apenas para subset de nós importantes se grafo for muito grande\n",
    "if G.number_of_nodes() > 1000:\n",
    "    print(\"Grafo grande detectado. Calculando betweenness para top 200 nós por grau...\")\n",
    "    # Pegar apenas os top nós por grau para calcular betweenness\n",
    "    top_nodes_by_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:200]\n",
    "    important_nodes = [node for node, _ in top_nodes_by_degree]\n",
    "    betweenness_centrality = nx.betweenness_centrality_subset(G, sources=important_nodes, targets=important_nodes, weight='weight')\n",
    "else:\n",
    "    print(\"Calculando centralidade de intermediação...\")\n",
    "    betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "\n",
    "top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# OTIMIZAÇÃO: Closeness centralidade também pode ser limitada\n",
    "if G.number_of_nodes() > 1000:\n",
    "    print(\"Calculando closeness centrality para subset...\")\n",
    "    closeness_centrality = {}\n",
    "    for node in important_nodes:\n",
    "        try:\n",
    "            closeness_centrality[node] = nx.closeness_centrality(G, u=node, distance='weight')\n",
    "        except:\n",
    "            closeness_centrality[node] = 0\n",
    "else:\n",
    "    print(\"Calculando centralidade de proximidade...\")\n",
    "    closeness_centrality = nx.closeness_centrality(G, distance='weight')\n",
    "\n",
    "top_closeness = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(\"\\nTop 5 por Centralidade de Grau:\")\n",
    "for user, score in top_degree:\n",
    "    print(f\"  {user}: {score:.6f}\")\n",
    "\n",
    "print(\"\\nTop 5 por Centralidade de Intermediação:\")\n",
    "for user, score in top_betweenness:\n",
    "    print(f\"  {user}: {score:.6f}\")\n",
    "\n",
    "print(\"\\nTop 5 por Centralidade de Proximidade:\")\n",
    "for user, score in top_closeness:\n",
    "    print(f\"  {user}: {score:.6f}\")\n",
    "\n",
    "# =========================================================\n",
    "# ETAPA 4: IDENTIFICAÇÃO DE POTENCIAIS ESPALHADORES\n",
    "# =========================================================\n",
    "print(\"\\n=== ETAPA 4: IDENTIFICAÇÃO DE ESPALHADORES DE FAKE NEWS ===\")\n",
    "\n",
    "# Combinar métricas para identificar potenciais espalhadores\n",
    "def identify_potential_spreaders(pagerank_dict, degree_dict, betweenness_dict, top_n=10):\n",
    "    \"\"\"Identifica potenciais espalhadores baseado em múltiplas métricas\"\"\"\n",
    "    all_users = set(pagerank_dict.keys())\n",
    "    \n",
    "    # Normalizar scores\n",
    "    max_pr = max(pagerank_dict.values())\n",
    "    max_deg = max(degree_dict.values())\n",
    "    max_bet = max(betweenness_dict.values())\n",
    "    \n",
    "    combined_scores = {}\n",
    "    for user in all_users:\n",
    "        pr_norm = pagerank_dict[user] / max_pr\n",
    "        deg_norm = degree_dict[user] / max_deg\n",
    "        bet_norm = betweenness_dict[user] / max_bet if max_bet > 0 else 0\n",
    "        \n",
    "        # Score combinado (pode ajustar os pesos)\n",
    "        combined_scores[user] = 0.4 * pr_norm + 0.3 * deg_norm + 0.3 * bet_norm\n",
    "    \n",
    "    return sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "potential_spreaders = identify_potential_spreaders(\n",
    "    pagerank_scores, degree_centrality, betweenness_centrality\n",
    ")\n",
    "\n",
    "print(\"Top 10 Potenciais Espalhadores de Fake News:\")\n",
    "for i, (user, score) in enumerate(potential_spreaders, 1):\n",
    "    print(f\"{i}. {user}: Score combinado {score:.6f}\")\n",
    "\n",
    "# =========================================================\n",
    "# ETAPA 5: VISUALIZAÇÕES\n",
    "# =========================================================\n",
    "print(\"\\n=== ETAPA 5: VISUALIZAÇÕES ===\")\n",
    "\n",
    "# 5.1 Distribuição de graus\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Distribuição de graus\n",
    "plt.subplot(2, 3, 1)\n",
    "degrees = [d for n, d in G.degree()]\n",
    "plt.hist(degrees, bins=50, alpha=0.7, edgecolor='black')\n",
    "plt.title('Distribuição de Graus')\n",
    "plt.xlabel('Grau')\n",
    "plt.ylabel('Frequência')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Subplot 2: Distribuição de PageRank\n",
    "plt.subplot(2, 3, 2)\n",
    "pr_values = list(pagerank_scores.values())\n",
    "plt.hist(pr_values, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "plt.title('Distribuição PageRank')\n",
    "plt.xlabel('Score PageRank')\n",
    "plt.ylabel('Frequência')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Subplot 3: Top usuários por PageRank\n",
    "plt.subplot(2, 3, 3)\n",
    "users_pr = [user[:10] + '...' for user, _ in top_pagerank[:5]]\n",
    "scores_pr = [score for _, score in top_pagerank[:5]]\n",
    "plt.barh(users_pr, scores_pr)\n",
    "plt.title('Top 5 PageRank')\n",
    "plt.xlabel('Score')\n",
    "\n",
    "# Subplot 4: Centralidades comparadas\n",
    "plt.subplot(2, 3, 4)\n",
    "top_users = [user for user, _ in top_pagerank[:5]]\n",
    "deg_scores = [degree_centrality[user] for user in top_users]\n",
    "bet_scores = [betweenness_centrality[user] for user in top_users]\n",
    "close_scores = [closeness_centrality[user] for user in top_users]\n",
    "\n",
    "x = np.arange(len(top_users))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, deg_scores, width, label='Grau', alpha=0.8)\n",
    "plt.bar(x, bet_scores, width, label='Intermediação', alpha=0.8)\n",
    "plt.bar(x + width, close_scores, width, label='Proximidade', alpha=0.8)\n",
    "\n",
    "plt.title('Comparação de Centralidades')\n",
    "plt.xlabel('Usuários (Top PageRank)')\n",
    "plt.ylabel('Score de Centralidade')\n",
    "plt.xticks(x, [u[:8] + '...' for u in top_users], rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 5: Tamanho das comunidades\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.bar(range(1, min(11, len(community_sizes)+1)), community_sizes[:10])\n",
    "plt.title('Tamanho das Comunidades')\n",
    "plt.xlabel('Comunidade')\n",
    "plt.ylabel('Número de Usuários')\n",
    "\n",
    "# Subplot 6: Rede das principais interações\n",
    "plt.subplot(2, 3, 6)\n",
    "# Criar subgrafo com apenas os top usuários\n",
    "top_20_users = [user for user, _ in potential_spreaders[:20]]\n",
    "G_sub = G.subgraph(top_20_users)\n",
    "\n",
    "pos = nx.spring_layout(G_sub, k=1, iterations=50)\n",
    "node_sizes = [pagerank_scores[node] * 10000 for node in G_sub.nodes()]\n",
    "\n",
    "nx.draw(G_sub, pos, node_size=node_sizes, node_color='red', \n",
    "        alpha=0.6, with_labels=False, edge_color='gray', arrows=True)\n",
    "plt.title('Rede dos Principais Espalhadores')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =========================================================\n",
    "# ETAPA 6: ANÁLISE DE RESULTADOS\n",
    "# =========================================================\n",
    "print(\"\\n=== ETAPA 6: ANÁLISE DE RESULTADOS ===\")\n",
    "\n",
    "print(f\"\\n📊 RESUMO DA ANÁLISE:\")\n",
    "print(f\"• Rede analisada com {G.number_of_nodes()} usuários e {G.number_of_edges()} interações\")\n",
    "print(f\"• Densidade da rede: {nx.density(G):.6f} (rede esparsa)\")\n",
    "print(f\"• {len(communities)} comunidades detectadas\")\n",
    "print(f\"• Usuário mais influente (PageRank): {top_pagerank[0][0]}\")\n",
    "print(f\"• Maior intermediador: {top_betweenness[0][0]}\")\n",
    "\n",
    "print(f\"\\n🎯 POTENCIAIS ESPALHADORES IDENTIFICADOS:\")\n",
    "print(\"Os usuários com maior potencial para espalhar fake news são aqueles que combinam:\")\n",
    "print(\"• Alto PageRank (influência)\")\n",
    "print(\"• Alta centralidade de grau (muitas conexões)\")\n",
    "print(\"• Alta centralidade de intermediação (ponte entre comunidades)\")\n",
    "\n",
    "print(f\"\\n💡 INSIGHTS PARA DETECÇÃO DE FAKE NEWS:\")\n",
    "print(\"1. Monitorar usuários com score combinado alto\")\n",
    "print(\"2. Focar em usuários que conectam diferentes comunidades\")\n",
    "print(\"3. Analisar padrões de disseminação rápida\")\n",
    "print(\"4. Verificar usuários com muitos retweets/mentions\")\n",
    "\n",
    "# Salvar resultados principais\n",
    "results_df = pd.DataFrame({\n",
    "    'user': [user for user, _ in potential_spreaders],\n",
    "    'combined_score': [score for _, score in potential_spreaders],\n",
    "    'pagerank': [pagerank_scores[user] for user, _ in potential_spreaders],\n",
    "    'degree_centrality': [degree_centrality[user] for user, _ in potential_spreaders],\n",
    "    'betweenness_centrality': [betweenness_centrality[user] for user, _ in potential_spreaders]\n",
    "})\n",
    "\n",
    "results_df.to_csv('potential_fake_news_spreaders.csv', index=False)\n",
    "print(f\"\\n✅ Resultados salvos em 'potential_fake_news_spreaders.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANÁLISE COMPLETA! 🎉\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcb6c16",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusões\n",
    "\n",
    "- A rede possui baixa densidade, o que é típico em redes sociais.\n",
    "- Foram detectadas várias comunidades, indicando agrupamentos naturais.\n",
    "- O PageRank, as centralidades de grau e de intermediação foram fundamentais para identificar os usuários mais influentes.\n",
    "- A exportação dos resultados permite análise no Gephi e facilita estudos posteriores.\n",
    "\n",
    "---\n",
    "\n",
    "**Análise completa finalizada.**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
